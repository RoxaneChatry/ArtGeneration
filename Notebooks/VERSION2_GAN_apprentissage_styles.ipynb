{"cells":[{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":5285,"status":"ok","timestamp":1645623366216,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"},"user_tz":-60},"id":"_NpTmcO9yvXk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8592f13b-a3d0-49be-fb24-2f19c2f1e9b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n"]}],"source":["import numpy as np\n","import math\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","import matplotlib.pyplot as plt\n","import cv2\n","import os\n","import random\n","from skimage.io import imread\n","from skimage.transform import resize\n","!pip install tensorflow_addons\n","import tensorflow_addons as tfa\n","\n","# tf.config.run_functions_eagerly(True)"]},{"cell_type":"markdown","metadata":{"id":"fN2eMfx70Mi3"},"source":["Objectif de ce notebook : créer le réseau (GAN) qui apprend un style de peinture (sur une selection de styles).\n","\n","https://www.kaggle.com/amyjang/monet-cyclegan-tutorial (tuto complet)\n","https://www.kaggle.com/shivansh002/gentle-introduction-to-gan (tuto basique + compréhensible)"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":43,"status":"ok","timestamp":1645623366217,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"},"user_tz":-60},"id":"LK4fkArp070p"},"outputs":[],"source":["PATH = \"/content/drive/Shareddrives/PFE artists/\"\n","DATA_PATH = \"/content/drive/Shareddrives/PFE artists/data/wikiart/base/\"\n","STYLES = [\"Photo\", \"Cubism\", \"Ukiyo_e\", \"Symbolism\", \"Expressionism\", \"Baroque\", \"Fauvism\"]\n","NB_CHANNELS = 3\n","IMG_SIZE = (128, 128)\n","IMG_SHAPE =  (*IMG_SIZE, NB_CHANNELS)"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":38,"status":"ok","timestamp":1645623366218,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"},"user_tz":-60},"id":"FX8FZtYF074T"},"outputs":[],"source":["# affichage d'une image\n","def display_image(image):\n","  plt.figure(figsize=(6,6))\n","  plt.imshow(image)\n","\n","def display_images(tab_images):\n","  # affiche plusieurs images contenues dans un tableau\n","  n = len(tab_images)\n","  n_rows = n//3 +1 if n%3!=0 else n//3\n","  figure, axs = plt.subplots(nrows=n_rows,ncols=3,figsize=(15,n_rows*5) )\n","  for i in range(n):\n","    img = tab_images[i]\n","    if n<=3 :\n","      axs[i].imshow(img)\n","    else :\n","      k = i//3\n","      j = i-k*3\n","      axs[k,j].imshow(img)\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-HGEES-cam_1"},"source":["Pour chaque style de peinture :    \n","- Générateur (réseau de neurones) qui recopie le style (entrainé sur les images du dossier correspondant)\n","- Discriminateur (réseau de neurones aussi) qui détermine si c'est un tableau de ce style ou non\n","     -> discriminateur utilisé aussi pour déterminé le style d'un tableau donné ? \n","- fonction qui entraine le modèle en boucle\n","\n","En théorie on peut utiliser les mêmes fonctions pour tous les styles, en les entrainant sur des bases d'images adaptées\n"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1645623366219,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"},"user_tz":-60},"id":"4WXWjbbqGBh1"},"outputs":[],"source":["# # Downsampling = réduit les dimensions du tableau (longeur et largeur) par 2\n","# def downsample(filters, size, norm=True):\n","#   initializer = tf.random_normal_initializer(0., 0.2)\n","  \n","#   model = keras.Sequential()\n","#   model.add(keras.layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initializer))\n","#   if norm:\n","#     model.add(keras.layers.BatchNormalization())\n","#   model.add(keras.layers.LeakyReLU())\n","#   return model\n","\n","# # Upsampling : augmente la dimension du tableau par 2\n","# def upsample(filters, size, drop=False):\n","#   initializer = tf.random_normal_initializer(0., 0.2)\n","\n","#   model = keras.Sequential()\n","#   model.add(keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n","#   model.add(keras.layers.BatchNormalization())\n","#   if drop:\n","#     model.add(keras.layers.Dropout(0.5))\n","#   model.add(keras.layers.ReLU())\n","\n","#   return model\n","\n","def upsample(filters, size, apply_dropout=False):\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n","\n","    result = keras.Sequential()\n","    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n","                                      padding='same',\n","                                      kernel_initializer=initializer,\n","                                      use_bias=False))\n","\n","    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n","\n","    if apply_dropout:\n","        result.add(layers.Dropout(0.5))\n","\n","    result.add(layers.ReLU())\n","\n","    return result\n","\n","def downsample(filters, size, apply_instancenorm=True):\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n","\n","    result = keras.Sequential()\n","    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n","                             kernel_initializer=initializer, use_bias=False))\n","\n","    if apply_instancenorm:\n","        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n","\n","    result.add(layers.LeakyReLU())\n","\n","    return result"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1645623366220,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"},"user_tz":-60},"id":"eA1BH5h2Jla6"},"outputs":[],"source":["# # Nouveau Générateur\n","# def generator(data_shape=IMG_SHAPE):\n","#   model = keras.Sequential(name=\"Generateur\")\n","#   # entrée\n","#   model.add(keras.layers.Input(shape=data_shape))\n","#   # downsampling\n","#   model.add(downsample(64, 4, False))\n","#   model.add(downsample(128, 4))\n","#   model.add(downsample(256, 4))\n","#   model.add(downsample(512, 4))\n","#   model.add(downsample(512, 4))\n","#   model.add(downsample(512, 4))\n","#   model.add(downsample(512, 4))\n","#   # model.add(downsample(512, 4))\n","#   # upsampling\n","#   # model.add(upsample(512, 4, True))\n","#   model.add(upsample(512, 4, True))\n","#   model.add(upsample(512, 4, True))\n","#   model.add(upsample(512, 4, True))\n","#   model.add(upsample(256, 4))\n","#   model.add(upsample(128, 4))\n","#   model.add(upsample(64, 4))\n","#   # sortie\n","#   initializer = tf.random_normal_initializer(0., 0.02)\n","#   model.add(keras.layers.Conv2DTranspose(NB_CHANNELS, 4, strides=2, padding='same', kernel_initializer=initializer, activation='relu'))\n","#   return model\n","\n","def discriminator(data_shape=IMG_SHAPE):\n","  model = keras.Sequential(name=\"Discriminateur\")\n","  model.add(keras.layers.Input(shape=data_shape))\n","  model.add(downsample(64, 4, False))\n","  model.add(downsample(128, 4))\n","  model.add(downsample(256, 4))\n","  model.add(keras.layers.ZeroPadding2D())\n","  initializer = tf.random_normal_initializer(0., 0.2)\n","  model.add(keras.layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False))\n","  model.add(keras.layers.BatchNormalization())\n","  model.add(keras.layers.LeakyReLU())\n","  model.add(keras.layers.ZeroPadding2D())\n","  model.add(keras.layers.Conv2DTranspose(1, 4, strides=1, kernel_initializer=initializer, activation='sigmoid'))\n","  model.add(keras.layers.Flatten())\n","  model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n","\n","  return model"]},{"cell_type":"code","source":["# def discriminator():\n","#     initializer = tf.random_normal_initializer(0., 0.02)\n","#     gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n","\n","#     inp = layers.Input(shape=IMG_SHAPE, name='input_image')\n","\n","#     x = inp\n","\n","#     down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n","#     down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n","#     down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n","\n","#     zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n","#     conv = layers.Conv2D(512, 4, strides=1,\n","#                          kernel_initializer=initializer,\n","#                          use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n","\n","#     norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n","\n","#     leaky_relu = layers.LeakyReLU()(norm1)\n","\n","#     zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n","\n","#     last = layers.Conv2D(1, 4, strides=1,\n","#                          kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n","\n","#     return tf.keras.Model(inputs=inp, outputs=last)\n","  \n","def generator():\n","    inputs = layers.Input(shape=IMG_SHAPE)\n","\n","    # bs = batch size\n","    down_stack = [\n","        downsample(64, 4, False), # (bs, 128, 128, 64)\n","        downsample(128, 4), # (bs, 64, 64, 128)\n","        downsample(256, 4), # (bs, 32, 32, 256)\n","        downsample(512, 4), # (bs, 16, 16, 512)\n","        downsample(512, 4), # (bs, 8, 8, 512)\n","        downsample(512, 4), # (bs, 4, 4, 512)\n","        downsample(512, 4), # (bs, 2, 2, 512)\n","        #downsample(512, 4), # (bs, 1, 1, 512)\n","    ]\n","\n","    up_stack = [\n","        upsample(512, 4, True), # (bs, 2, 2, 1024)\n","        upsample(512, 4, True), # (bs, 4, 4, 1024)\n","        upsample(512, 4, True), # (bs, 8, 8, 1024)\n","        #upsample(512, 4), # (bs, 16, 16, 1024)\n","        upsample(256, 4), # (bs, 32, 32, 512)\n","        upsample(128, 4), # (bs, 64, 64, 256)\n","        upsample(64, 4), # (bs, 128, 128, 128)\n","    ]\n","\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    last = layers.Conv2DTranspose(NB_CHANNELS, 4,\n","                                  strides=2,\n","                                  padding='same',\n","                                  kernel_initializer=initializer,\n","                                  activation='tanh') # (bs, 256, 256, 3)\n","    lastlast = layers.Lambda(tf.math.abs)\n","    x = inputs\n","\n","    # Downsampling through the model\n","    skips = []\n","    for down in down_stack:\n","        x = down(x)\n","        skips.append(x)\n","\n","    skips = reversed(skips[:-1])\n","\n","    # Upsampling and establishing the skip connections\n","    for up, skip in zip(up_stack, skips):\n","        x = up(x)\n","        x = layers.Concatenate()([x, skip])\n","\n","    x = last(x)\n","    #x = lastlast(x)\n","\n","    return keras.Model(inputs=inputs, outputs=x)"],"metadata":{"id":"0ygo8dPNnOCu","executionInfo":{"status":"ok","timestamp":1645623366221,"user_tz":-60,"elapsed":29,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1645623366221,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"},"user_tz":-60},"id":"yBLPyK2o3FRU"},"outputs":[],"source":["from tensorflow.python.keras.utils.data_utils import Sequence\n","\n","def gen_dataset(path, batch_size=32):\n","  # génère le dataset sous forme d'iterateur\n","  dataset = tf.keras.utils.image_dataset_from_directory(\n","    directory = path,\n","    labels = None,\n","    color_mode = 'rgb',\n","    image_size=IMG_SIZE,\n","    shuffle = True,\n","    batch_size = batch_size,\n","  )\n","  # mutliplication des données et mélange\n","  dataset.repeat(2)\n","  dataset.shuffle(1000)\n","  # normalisation des données\n","  normalization_layer = tf.keras.layers.Rescaling(1./255)\n","  normalized_ds = dataset.map(lambda x: (normalization_layer(x)))\n","  return normalized_ds #.as_numpy_iterator()\n","\n","def gen_datasets_all(batch_size=32, styles=STYLES):\n","  datasets_all = []\n","  for style in styles:\n","    path = DATA_PATH+style\n","    dataset = gen_dataset(path, batch_size)\n","    datasets_all.append(dataset)\n","  return tuple(datasets_all)\n","\n","\n","class ZippedDatasets(Sequence):\n","\n","    def __init__(self, batch_size=32, path=DATA_PATH, styles=STYLES):\n","        self.path = path\n","        self.styles = styles\n","        self.batch_size = batch_size\n","        self.datasets = {}\n","        # récup de toutes les images sous forme de datagenerator pour chaque style\n","        for style in self.styles : \n","          self.datasets[style] = gen_dataset(self.path+style, self.batch_size)\n","\n","    def __len__(self):\n","      # chacun des datasets a une cardinality = nb d'épochs nécessaires pour balayer toutes les données\n","      # pour éviter le dernier epoch (qui ne contient pas un batch complet de données), \n","        # on set notre len==cardinality à min -1 \n","      return min(map(lambda x : x.cardinality().numpy(), self.datasets.values()))\n","\n","    def __getitem__(self, idx):\n","      batch = {}\n","      for style in self.styles:\n","        batch[style] = self.datasets[style].batch(self.batch_size, drop_remainder=True)\n","      return batch\n","    \n","    # def on_epoch_end(self):\n","    #   # permuter les données ? Surement déjà fait pas les datagenarators de base\n","\n","\n","\n","# def test_generator(generator, images):\n","#   # affiche plusieurs images contenues dans un tableau\n","#   n = len(images)\n","#   predictions = generator(images)\n","#   figure, axs = plt.subplots(nrows=n,ncols=2,figsize=(10,n*5) )\n","#   for i in range(n):\n","#     img = images[i]\n","#     pred = predictions[i]\n","#     if n==1 :\n","#       axs[0].imshow(img)\n","#       axs[1].imshow(pred)\n","#     else :\n","#       axs[i,0].imshow(img)\n","#       axs[i,1].imshow(pred)\n","#   plt.show()\n","\n","# min(map(lambda x : x.cardinality().numpy(), ZippedDatasets().datasets.values()))\n"]},{"cell_type":"code","source":["# Métriques (tout sous forme de loss = à minimiser)\n","\n","def loss_generator(disc_fake):\n","  # entrée : évaluation du discriminateur sur les fausses images générées par le générateur\n","  # objectif : si générateur parfait, disc_fake doit être un vecteur de 1\n","  target = tf.ones_like(disc_fake)\n","  loss = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)(target, disc_fake)\n","  return loss\n","\n","def loss_discriminator(disc_real, disc_fake):\n","  # entrée : évaluation du discriminateur sur les fausses images et sur les vraies\n","  # objectif : disc_fake doit être un vecteur de 0 // disc_real un vecteur de 1\n","  uns = tf.ones_like(disc_real)\n","  zeros = tf.zeros_like(disc_fake)\n","  loss_real = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)(uns, disc_real)\n","  loss_fake = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)(zeros, disc_fake)\n","  return (loss_real+loss_fake)\n","  #return tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)(uns, disc_real)\n","\n","def loss_cycle(original, cycled):\n","  # loss par passage d'une image dans un cycle\n","  # entrée : l'image originale, et la version passée dans 2 générateurs à la suite\n","  # objectif : les images doivent être identiques\n","  return tf.reduce_mean(tf.abs(original - cycled))\n","\n","def loss_identity(original, processed):\n","  # loss par passage d'une image dans le générateur de son vrai style\n","  # objectif : l'image ne doit pas avoir été modifiée\n","  # entrée : l'image d'origine et l'image traité par le générateur\n","  return tf.reduce_mean(tf.abs(original - processed))\n"],"metadata":{"id":"Uvxu20gDirIE","executionInfo":{"status":"ok","timestamp":1645623366222,"user_tz":-60,"elapsed":25,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# initialisation des modèles\n","def build_models(styles=STYLES, batch_size=32, img_shape=IMG_SHAPE):\n","  discriminators = {}\n","  generators = {}\n","  # pour chacun des styles étudiés, on initialise un générateur et un discriminateur\n","  for style in styles:\n","    # ex: generators[\"Photo\"] transforme une image en photo\n","    # ex: discriminators[\"Photo\"] détermine si une image est une photo\n","    generators[style] = generator()\n","    #generators[style].build(input_shape=(batch_size,*img_shape))\n","    discriminators[style] = discriminator()\n","    #discriminators[style].build(input_shape=(batch_size, *img_shape))\n","  return generators, discriminators\n","\n","# définition des optimizers\n","def build_optimizers(styles=STYLES):\n","  gen_optimizers = {}\n","  disc_optimizers = {}\n","  for style in styles:\n","    gen_optimizers[style] = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","    disc_optimizers[style] = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","  return gen_optimizers, disc_optimizers"],"metadata":{"id":"MjG8DcQ3K2W7","executionInfo":{"status":"ok","timestamp":1645623366224,"user_tz":-60,"elapsed":24,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["from tensorflow.python.ops.gen_math_ops import TruncateMod\n","# Training fait en override les méthodes de keras\n","class CycleGan(keras.Model):\n","  def __init__(self, generators, discriminators, lambda_cycle=10, styles=STYLES, batch_size=32):\n","    super(CycleGan, self).__init__()\n","    self.generators = generators\n","    self.discriminators = discriminators\n","    self.lambda_cycle = lambda_cycle\n","    self.styles = styles\n","    self.batch_size = batch_size\n","    # self.datasets = datasets\n","  \n","  def compile(self, disc_optimizers, gen_optimizers, loss_generator_fn, loss_discriminator_fn, loss_cycle_fn, loss_identity_fn):\n","    super(CycleGan, self).compile()\n","    self.disc_optimizers = disc_optimizers\n","    self.gen_optimizers = gen_optimizers\n","    self.loss_generator_fn = loss_generator_fn\n","    self.loss_discriminator_fn = loss_discriminator_fn\n","    self.loss_cycle_fn = loss_cycle_fn\n","    self.loss_identity_fn = loss_identity_fn\n","    # stockage des losses\n","    self.gen_loss = {}\n","    self.disc_loss = {}\n","    self.cycle_loss = {}\n","    self.id_loss = {}\n","    # initialiser les poids des optimizers\n","    # for style in self.styles:\n","    #   init_g = tf.compat.v1.variables_initializer(gen_optimizers[style].variables())\n","    #   #init_g.run()\n","    #   init_d = tf.compat.v1.variables_initializer(disc_optimizers[style].variables())\n","    #   gen_optimizers[style].set_weights(??)\n","    #   disc_optimizers[style].set_weights(??)\n","  \n","  def call(self, inputs, training=False):\n","    #super(CycleGan, self).__call__(inputs)\n","    # call tous les modeles puis return leurs résultats ? \n","    inputs = tf.reshape(inputs, (1,*inputs.shape))\n","    res = {}\n","    for i in range(len(self.styles)):\n","      style = self.styles[i]\n","      res[style] = self.discriminators[style].call(self.generators[style].call(inputs))\n","    return res\n","\n","  def build(self, input_shape):\n","    super(CycleGan, self).build(input_shape)\n","    for style in self.styles:\n","      self.generators[style].build(input_shape)\n","      self.discriminators[style].build(input_shape)\n","    #   # initialisation des losses\n","    #   self.gen_loss[style] = self.loss_generator_fn(tf.zeros(self.batch_size))\n","    #   self.disc_loss[style] = self.loss_discriminator_fn(tf.zeros(self.batch_size), tf.ones(self.batch_size))\n","    #   self.cycle_loss[style] = self.loss_cycle_fn(np.zeros(input_shape), np.ones(input_shape))\n","    #   self.id_loss[style] = self.loss_identity_fn(np.zeros(input_shape), np.ones(input_shape))\n","    #   # premier évaluation des optimizers (pour éviter les ValueErrors dues à des tf.Variables créées dans la boucle)\n","    # with tf.GradientTape(persistent=False) as tape:\n","    #   for style in self.styles:\n","    #     tape.watch(self.generators[style].trainable_variables)\n","    #     tape.watch(self.generators[style].trainable_variables)\n","    # for style in self.styles:\n","    #   generator_gradients = tape.gradient(self.gen_loss[style], self.generators[style].trainable_variables)\n","    #   self.gen_optimizers[style].apply_gradients(zip(generator_gradients, self.generators[style].trainable_variables))\n","    #   discriminator_gradients = tape.gradient(self.disc_loss[style], self.discriminators[style].trainable_variables)\n","    #   self.disc_optimizers[style].apply_gradients(zip(discriminator_gradients, self.discriminators[style].trainable_variables))\n","  \n","  def save(self):\n","    path = PATH+\"models/\"\n","    for style in self.styles:\n","      path_style = path+style\n","      self.generators[style].save(path_style+\"/generator.h5\")\n","      self.discriminators[style].save(path_style+\"/discriminator.h5\")\n","  \n","  def __getitem__(self, batch_data):\n","    # batch_data = self.datasets.batch(batch_size= self.batch_size, drop_remainder=True)\n","    batch = {}\n","    i=0\n","    for style in self.styles:\n","      batch[style] = batch_data[i]\n","      i+=1\n","    return batch\n","\n","\n","  def train_step(self, batch_data):\n","    \"\"\" batch = liste des batchs de données pour chaque style \"\"\"\n","    n_styles = len(self.styles)\n","    batch = self.__getitem__(batch_data)\n","    # batch = batch_data\n","\n","    with tf.GradientTape(persistent=True) as tape:\n","      # on selectionne un style k aléatoire à prendre pour référence dans cet epoch\n","      k = np.random.randint(n_styles)\n","      style_k = self.styles[k]\n","      # On entraine chaque style j!= k sur style_k\n","      for j in range(n_styles):\n","        if j!=k :\n","          style_j = self.styles[j]\n","          # GENERATEUR\n","          # k to j to k\n","          # transforme une image de style k en style j\n","          fake_j = self.generators[style_j](batch[style_k], training=True)\n","          # retour au style k\n","          cycled_k = self.generators[style_k](fake_j, training=True)\n","\n","          # DISCRIMINATEUR\n","          disc_real_j = self.discriminators[style_j](batch[style_j], training=True)\n","          disc_fake_j = self.discriminators[style_j](fake_j, training=True)\n","\n","          # LOSS\n","          self.gen_loss[style_j] = (self.loss_generator_fn(disc_fake_j))\n","          self.disc_loss[style_j] = (self.loss_discriminator_fn(disc_real_j, disc_fake_j))\n","          self.cycle_loss[style_j] = (self.loss_cycle_fn(batch[style_k], cycled_k))\n","          self.id_loss[style_j] = (self.loss_identity_fn(batch[style_j], fake_j))\n","\n","    # GRADIENTS\n","    for j in range(n_styles):\n","      if j!=k:\n","        style_j = self.styles[j]\n","        generator_gradients = tape.gradient(self.gen_loss[style_j], self.generators[style_j].trainable_variables)\n","        self.gen_optimizers[style_j].apply_gradients(zip(generator_gradients, self.generators[style_j].trainable_variables))\n","        discriminator_gradients = tape.gradient(self.disc_loss[style_j], self.discriminators[style_j].trainable_variables)\n","        self.disc_optimizers[style_j].apply_gradients(zip(discriminator_gradients, self.discriminators[style_j].trainable_variables))\n","\n","    metriques = {\n","        \"k\" : style_k,\n","    }\n","    #self.disc_loss.update(self.gen_loss)\n","    #metriques.update(self.disc_loss)\n","    for j in range(n_styles):\n","      if j!=k:\n","        metriques[\"gen_\"+self.styles[j]] = self.gen_loss[self.styles[j]]\n","        metriques[\"disc_\"+self.styles[j]] = self.disc_loss[self.styles[j]]\n","    #print(\"k : \", style_k)\n","    #print(self.disc_loss)\n","    return metriques\n","\n"],"metadata":{"id":"_9Dm747GrD2W","executionInfo":{"status":"ok","timestamp":1645623366583,"user_tz":-60,"elapsed":381,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["#D['Photos'](images, training=True) # retourne la prédiction et entraine le modèle au passage ? \n","st = [\"Photo\", \"Symbolism\", \"Expressionism\", \"Baroque\"]\n","datasets = gen_datasets_all(batch_size=32, styles=st)\n","# data = ZippedDatasets(styles=st)\n","G, D = build_models(st)\n","gen_optimizers, disc_optimizers = build_optimizers(st)"],"metadata":{"id":"QdTkV6A-PCn0","executionInfo":{"status":"ok","timestamp":1645623374980,"user_tz":-60,"elapsed":8405,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7180d36c-b8b8-4764-ac76-e096602ba3dc"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 7038 files belonging to 1 classes.\n","Found 4500 files belonging to 1 classes.\n","Found 6725 files belonging to 1 classes.\n","Found 4221 files belonging to 1 classes.\n"]}]},{"cell_type":"code","execution_count":41,"metadata":{"id":"XItbWh8r1MXI","executionInfo":{"status":"ok","timestamp":1645623425734,"user_tz":-60,"elapsed":535,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"}}},"outputs":[],"source":["modele = CycleGan(G, D, styles=st)\n","modele.compile(disc_optimizers, gen_optimizers, loss_generator, loss_discriminator, loss_cycle, loss_identity)"]},{"cell_type":"code","source":["# modele.generators['Photo'].summary()\n","# modele.build(input_shape=IMG_SHAPE)"],"metadata":{"id":"yWt4QWebNtm1","executionInfo":{"status":"ok","timestamp":1645623375420,"user_tz":-60,"elapsed":458,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# entrainement\n","history = modele.fit(\n","    # data,\n","    tf.data.Dataset.zip(datasets),\n","    epochs=5,\n","    steps_per_epoch=2,\n","    verbose=True,\n",")"],"metadata":{"id":"woqDPIX9kvib","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"06451863-b4d9-4477-d274-ba6a32e0515e","executionInfo":{"status":"error","timestamp":1645623863467,"user_tz":-60,"elapsed":436668,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"}}},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7f86da477560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-c5228f4d19cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' - %s:'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m           \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' %.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'bytes' and 'int'"]}]},{"cell_type":"code","source":["modele.save()"],"metadata":{"id":"ucnbuVNQKG7B","executionInfo":{"status":"aborted","timestamp":1645623415644,"user_tz":-60,"elapsed":32,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images = gen_dataset(path=DATA_PATH, batch_size=3).as_numpy_iterator()"],"metadata":{"id":"zq4dBCs0Ext2","executionInfo":{"status":"aborted","timestamp":1645623415646,"user_tz":-60,"elapsed":32,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2G8sNVGf6QeA","executionInfo":{"status":"aborted","timestamp":1645623415647,"user_tz":-60,"elapsed":32,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"}}},"outputs":[],"source":["from numpy.core.numerictypes import maximum_sctype\n","# ajouter fonction pour sauvegarder tous les modèles\n","G = modele.generators[\"Symbolism\"]\n","pred = G.predict(images.next())\n","print(pred.min(), pred.max())\n","\n","def test_generator2(generator, images):\n","  # affiche plusieurs images contenues dans un tableau\n","  n = len(images)\n","  predictions = (generator(images)).numpy()\n","  min = predictions.min()\n","  max = predictions.max()\n","  rescale = lambda x : (x-min)/(max-min)\n","  print(min, max)\n","  # predictions = (predictions * 127.5 + 127.5).astype(np.uint8)\n","  predictions = rescale(predictions)\n","  print(predictions.min(), predictions.max())\n","  figure, axs = plt.subplots(nrows=n,ncols=2,figsize=(10,n*5) )\n","  for i in range(n):\n","    img = images[i]\n","    pred = (predictions[i])\n","    if n==1 :\n","      axs[0].imshow(img)\n","      axs[1].imshow(pred)\n","    else :\n","      axs[i,0].imshow(img)\n","      axs[i,1].imshow(pred)\n","  plt.show()\n","\n","test_generator2(G, images.next())"]},{"cell_type":"code","source":["D = modele.discriminators[\"Symbolism\"]\n","D.predict(G.predict(images.next()))"],"metadata":{"id":"r-VCojs0F7s6","executionInfo":{"status":"aborted","timestamp":1645623415648,"user_tz":-60,"elapsed":32,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cZhfLlS2-7d","executionInfo":{"status":"aborted","timestamp":1645623415648,"user_tz":-60,"elapsed":30,"user":{"displayName":"Roxane Chatry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14146719894395936762"}}},"outputs":[],"source":["# tracer de l'évolution des métriques au long de l'entrainement\n","def plot_learning_curves(acc, loss):\n","    #epochs = range(len(acc))\n","\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,5))\n","    \n","    ax1.plot(acc)\n","    ax1.set_title(\"Accuracy\")\n","    ax1.set_ylabel(\"Accuracy\")\n","    ax1.set_xlabel(\"Epoch\")\n","    \n","    ax2.plot(loss)\n","    ax2.set_title(\"Loss\")\n","    ax2.set_ylabel('Loss')\n","    ax2.set_xlabel('Epoch')\n","\n","    fig.show()\n","\n","plot_learning_curves(a_acc, a_loss)\n","plot_learning_curves(d_acc, d_loss)"]},{"cell_type":"markdown","metadata":{"id":"Sw_PJhqYDD_J"},"source":["- Expressionnism\n","- Baroque\n","- Symbolisme\n","- Cubisme\n","- Ukiyo_e\n","- Fauvisme\n","\n","\n","Pointillisme (peu de data)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"VERSION2_GAN_apprentissage_styles.ipynb","provenance":[],"mount_file_id":"1VzHOXletttUvTT3oaswFClfAHzDXCIdO","authorship_tag":"ABX9TyML+EFShxj+w82oRi2UT02r"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}